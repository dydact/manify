{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn how to perform clustering in this section. First, we will write the evaluation function, which provides various clustering metrics. For each metric, the closer the value is to 1, the better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    normalized_mutual_info_score,\n",
    "    adjusted_rand_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from collections import Counter\n",
    "\n",
    "def compute_label_alignment(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    mapping = {row: col for row, col in zip(row_ind, col_ind)}\n",
    "    y_pred_aligned = np.array([mapping[label] for label in y_pred])\n",
    "    acc = sum(w[i, j] for i, j in zip(row_ind, col_ind)) / y_pred.size\n",
    "    return acc, y_pred_aligned\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    total = 0\n",
    "    for cluster in np.unique(y_pred):\n",
    "        indices = np.where(y_pred == cluster)[0]\n",
    "        true_labels = y_true[indices]\n",
    "        most_common = Counter(true_labels).most_common(1)\n",
    "        if most_common:\n",
    "            total += most_common[0][1]\n",
    "    return total / len(y_true)\n",
    "\n",
    "def evaluate(y_true, y_pred, method='macro'):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    # ACC & aligned labels\n",
    "    acc, y_pred_aligned = compute_label_alignment(y_true, y_pred)\n",
    "\n",
    "    # Metrics\n",
    "    nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "    purity = purity_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred_aligned, average=method, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_aligned, average=method, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_aligned, average=method, zero_division=0)\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "\n",
    "    return np.array([acc, nmi, purity, f1, precision, recall, ari])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from manify.manifolds import ProductManifold\n",
    "from manify.clustering.fuzzy_kmeans import RiemannianFuzzyKMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate a **Product Manifold** using the following method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the signature: a 3-factor manifold\n",
    "import numpy as np\n",
    "#    (curvature, dimension)\n",
    "signature = [\n",
    "    (0.0, 4),   # R^2 (Euclidean space)\n",
    "    (1.0, 4),   # S^2 (Spherical space)\n",
    "    (-1.0, 4),  # H^2 (Hyperbolic space)\n",
    "]\n",
    "\n",
    "# 2. Construct the ProductManifold (without stereographic projection)\n",
    "P = ProductManifold(signature, device=\"cpu\", stereographic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting param\n",
    "n_clusters = 3\n",
    "seed = 0\n",
    "opt = 'adan'\n",
    "lr = .01\n",
    "tol = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generate data using gaussian_mixture\n",
    "#    - num_points=500: sample 500 points\n",
    "#    - num_classes=n_clusters: generate n_clusters class labels (for clustering)\n",
    "#    - seed=seed: fix the random seed for reproducibility\n",
    "X, y_true = P.gaussian_mixture(\n",
    "    num_points=500,\n",
    "    num_classes=n_clusters,\n",
    "    seed=seed,\n",
    "    task=\"classification\",\n",
    "    cov_scale_points=.1 # <--- try decreasing this value\n",
    ")\n",
    "y_true = np.array(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `RiemannianFuzzyKMeans` algorithm from the `fuzzy_kmeans` module in the `manify` clustering package to perform clustering on a manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFK iter 1, loss=1911.6559\n",
      "RFK iter 2, loss=1909.2720\n",
      "RFK iter 3, loss=1907.2013\n",
      "RFK iter 4, loss=1905.3054\n",
      "RFK iter 5, loss=1903.5615\n",
      "RFK iter 6, loss=1901.9583\n",
      "RFK iter 7, loss=1900.4875\n",
      "RFK iter 8, loss=1899.1450\n",
      "RFK iter 9, loss=1897.9203\n",
      "RFK iter 10, loss=1896.8094\n",
      "RFK iter 11, loss=1895.7996\n",
      "RFK iter 12, loss=1894.8835\n",
      "RFK iter 13, loss=1894.0524\n",
      "RFK iter 14, loss=1893.2966\n",
      "RFK iter 15, loss=1892.6089\n",
      "RFK iter 16, loss=1891.9822\n",
      "RFK iter 17, loss=1891.4094\n",
      "RFK iter 18, loss=1890.8866\n",
      "RFK iter 19, loss=1890.4080\n",
      "RFK iter 20, loss=1889.9674\n",
      "RFK iter 21, loss=1889.5638\n",
      "RFK iter 22, loss=1889.1930\n",
      "RFK iter 23, loss=1888.8501\n",
      "RFK iter 24, loss=1888.5347\n",
      "RFK iter 25, loss=1888.2428\n",
      "RFK iter 26, loss=1887.9733\n",
      "RFK iter 27, loss=1887.7229\n",
      "RFK iter 28, loss=1887.4913\n",
      "RFK iter 29, loss=1887.2771\n",
      "RFK iter 30, loss=1887.0775\n",
      "RFK iter 31, loss=1886.8916\n",
      "RFK iter 32, loss=1886.7195\n",
      "RFK iter 33, loss=1886.5583\n",
      "RFK iter 34, loss=1886.4092\n",
      "RFK iter 35, loss=1886.2698\n",
      "RFK iter 36, loss=1886.1398\n",
      "RFK iter 37, loss=1886.0182\n",
      "RFK iter 38, loss=1885.9049\n",
      "RFK iter 39, loss=1885.7992\n",
      "RFK iter 40, loss=1885.7002\n",
      "RFK iter 41, loss=1885.6074\n",
      "RFK iter 42, loss=1885.5209\n",
      "RFK iter 43, loss=1885.4402\n",
      "RFK iter 44, loss=1885.3644\n",
      "RFK iter 45, loss=1885.2936\n",
      "RFK iter 46, loss=1885.2272\n",
      "RFK iter 47, loss=1885.1655\n",
      "RFK iter 48, loss=1885.1078\n",
      "RFK iter 49, loss=1885.0532\n",
      "RFK iter 50, loss=1885.0027\n",
      "RFK iter 51, loss=1884.9554\n",
      "RFK iter 52, loss=1884.9111\n",
      "RFK iter 53, loss=1884.8702\n",
      "RFK iter 54, loss=1884.8313\n",
      "RFK iter 55, loss=1884.7960\n",
      "RFK iter 56, loss=1884.7618\n",
      "RFK iter 57, loss=1884.7318\n",
      "RFK iter 58, loss=1884.7024\n",
      "RFK iter 59, loss=1884.6760\n",
      "RFK iter 60, loss=1884.6500\n",
      "RFK iter 61, loss=1884.6274\n",
      "RFK iter 62, loss=1884.6057\n",
      "RFK iter 63, loss=1884.5859\n",
      "RFK iter 64, loss=1884.5674\n",
      "RFK iter 65, loss=1884.5500\n",
      "RFK iter 66, loss=1884.5342\n",
      "RFK iter 67, loss=1884.5195\n",
      "RFK iter 68, loss=1884.5063\n",
      "RFK iter 69, loss=1884.4939\n",
      "RFK iter 70, loss=1884.4825\n",
      "RFK iter 71, loss=1884.4719\n",
      "RFK iter 72, loss=1884.4620\n",
      "RFK iter 73, loss=1884.4526\n",
      "RFK iter 74, loss=1884.4442\n",
      "RFK iter 75, loss=1884.4366\n",
      "RFK iter 76, loss=1884.4297\n",
      "RFK iter 77, loss=1884.4235\n",
      "RFK iter 78, loss=1884.4174\n",
      "RFK iter 79, loss=1884.4120\n",
      "RFK iter 80, loss=1884.4073\n",
      "RFK iter 81, loss=1884.4026\n",
      "RFK iter 82, loss=1884.3982\n",
      "RFK iter 83, loss=1884.3947\n",
      "RFK iter 84, loss=1884.3909\n",
      "RFK iter 85, loss=1884.3877\n",
      "RFK iter 86, loss=1884.3842\n",
      "RFK iter 87, loss=1884.3817\n",
      "RFK iter 88, loss=1884.3794\n",
      "RFK iter 89, loss=1884.3767\n",
      "RFK iter 90, loss=1884.3749\n",
      "RFK iter 91, loss=1884.3727\n",
      "RFK iter 92, loss=1884.3707\n",
      "RFK iter 93, loss=1884.3694\n",
      "RFK iter 94, loss=1884.3676\n",
      "RFK iter 95, loss=1884.3666\n",
      "RFK iter 96, loss=1884.3644\n",
      "RFK iter 97, loss=1884.3638\n",
      "RFK iter 98, loss=1884.3623\n",
      "RFK iter 99, loss=1884.3612\n",
      "RFK iter 100, loss=1884.3602\n",
      "RFK iter 101, loss=1884.3591\n",
      "RFK iter 102, loss=1884.3583\n",
      "RFK iter 103, loss=1884.3574\n",
      "RFK iter 104, loss=1884.3569\n",
      "RFK iter 105, loss=1884.3561\n",
      "RFK iter 106, loss=1884.3547\n",
      "RFK iter 107, loss=1884.3544\n",
      "RFK iter 108, loss=1884.3538\n",
      "RFK iter 109, loss=1884.3536\n",
      "RFK iter 110, loss=1884.3523\n",
      "RFK iter 111, loss=1884.3518\n",
      "RFK iter 112, loss=1884.3511\n",
      "RFK iter 113, loss=1884.3511\n"
     ]
    }
   ],
   "source": [
    "model = RiemannianFuzzyKMeans(n_clusters, \n",
    "            manifold=P,\n",
    "            random_state=seed,  \n",
    "            max_iter=1000,\n",
    "            tol=tol,\n",
    "            optimizer=opt,\n",
    "            lr=lr,\n",
    "            verbose=True)\n",
    "labels = model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we don't use a manifold-based clustering method and instead apply standard KMeans? We'll compare the results to evaluate the performance difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "# Fit the data\n",
    "kmeans.fit(X)\n",
    "# Get the cluster labels from kmeans\n",
    "labels_km = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.998      0.98869808 0.998      0.99811609 0.99801587 0.99822695\n",
      "  0.99363694]]\n",
      "[[0.44       0.07972898 0.444      0.30688818 0.33132184 0.40742235\n",
      "  0.03685404]]\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(y_true, labels).reshape(1, -1)\n",
    "result2 = evaluate(y_true, labels_km).reshape(1, -1)\n",
    "print(result)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of **Riemannian Fuzzy KMeans** seems to be much better than that of standard **KMeans**. Let's try adjusting some parameters to see if we can improve or better understand the results!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
